{"desc_call_llm_generate_reply": "Call LLM to generate reply.", "title_llm_settings": "LLM Settings", "label_llm_model": "LLM Model", "btn_advanced_settings": "Advanced Settings", "label_prompt_role": "Prompt Role", "tip_prompt_role_desc": "When calling LLM, the prompt will be concatenated under the corresponding role.", "label_system_role_example": "System role example:", "sample_custom_prompt": "Custom prompt", "label_system_role": "System (System)", "label_user_role": "User (User)", "label_temperature": "Temperature", "tip_temperature_desc": "Lower temperature makes answers more precise. Higher temperature makes answers more diverse.", "label_max_token": "Max Tokens", "tip_max_token_desc": "Maximum tokens for question+answer. If the answer is truncated, increase this value.", "label_deep_thinking": "Deep Thinking", "tip_deep_thinking_desc": "When enabled, the LLM will use deep thinking mode.", "label_context_pair": "Context Count", "tip_context_pair_desc": "Number of historical chat rounds in the prompt. Set to 0 to exclude chat history. Maximum 50 rounds. Note: More history consumes more tokens.", "label_prompt": "Prompt", "btn_import_from_prompt_library": "Import from Prompt Library", "ph_input_message_content": "Enter message content, type \"/\" to insert variables", "msg_input_insert_variable": "Type / to insert variables", "label_knowledge_base_reference": "Knowledge Base Reference", "ph_select": "Please select", "label_input": "Input", "label_output": "Output", "label_ai_reply_content": "AI Reply Content"}