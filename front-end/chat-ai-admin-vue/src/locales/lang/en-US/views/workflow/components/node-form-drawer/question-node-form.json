{"desc_question_classification": "The LLM determines which category the user message belongs to. Different categories take different branches.", "label_input": "Input", "label_llm_model": "LLM Model", "btn_advanced_settings": "Advanced Settings", "label_prompt_role": "Prompt Role", "tip_prompt_role_desc": "When calling LLM, the prompt will be concatenated under the corresponding role.", "label_system_role_example": "System role example:", "sample_custom_prompt": "Custom prompt", "label_system_role": "System (System)", "label_user_role": "User (User)", "label_temperature": "Temperature", "tip_temperature_desc": "Lower temperature makes answers more precise. Higher temperature makes answers more diverse.", "label_max_token": "Max Tokens", "tip_max_token_desc": "Maximum tokens for question+answer. If the answer is truncated, increase this value.", "label_deep_thinking": "Deep Thinking", "tip_deep_thinking_desc": "When enabled, the LLM will use deep thinking mode.", "label_context_pair": "Context Count", "tip_context_pair_desc": "Number of historical chat rounds in the prompt. Set to 0 to exclude chat history. Maximum 50 rounds. Note: More history consumes more tokens.", "label_user_question": "User Question", "label_question_classification_settings": "Question Category Settings", "label_default_category": "Default Category", "btn_add_question_category": "Add Question Category", "ph_select": "Please select", "ph_input": "Please enter"}