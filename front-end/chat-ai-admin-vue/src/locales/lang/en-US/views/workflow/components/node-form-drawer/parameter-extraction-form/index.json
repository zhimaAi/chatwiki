{"desc_param_extraction": "Use LLM to extract specified parameters from user messages and output to next node", "label_input": "Input", "label_llm_model": "LLM Model", "btn_advanced_settings": "Advanced Settings", "label_prompt_role": "Prompt Role", "tip_prompt_role_desc": "When calling LLM, the prompt will be concatenated under the corresponding role.", "label_system_role_example": "System role example:", "sample_custom_prompt": "Custom prompt", "label_system_role": "System (System)", "label_user_role": "User (User)", "label_temperature": "Temperature", "tip_temperature": "Lower temperature makes answers more rigorous. Higher temperature makes answers more diverse.", "label_max_token": "Max Token", "tip_max_token": "Maximum tokens for question + answer. If answer is truncated, increase this value", "label_deep_thinking": "Deep Thinking", "tip_deep_thinking": "When enabled, LLM will be called in deep thinking mode", "label_context_count": "Context Count", "tip_context_count": "Number of historical conversation rounds included in prompt. Set to 0 to exclude history. Maximum 50 rounds. Note: more history consumes more tokens.", "label_prompt": "Prompt", "ph_input_message": "Enter message content, type '/' to insert variable", "tip_input_variable": "Type '/' to insert variable", "label_user_question": "User Question", "ph_select": "Please select", "label_output_extract": "Output (Extract Output Fields)", "label_param_key": "Parameter Key", "label_type": "Type", "ph_input": "Please enter", "btn_add_param": "Add Parameter"}