{"label_input": "Input", "label_llm_model": "LLM Model", "btn_advanced_settings": "Advanced Settings", "label_temperature": "Temperature", "msg_temperature_tooltip": "Lower temperature means more precise answers. Higher temperature means more diverse answers.", "label_max_token": "Max Token", "msg_max_token_tooltip": "Maximum token count for question + answer. If answer is truncated, increase this value", "label_deep_thinking": "Deep Thinking", "msg_deep_thinking_tooltip": "When enabled, the model will use deep thinking mode", "label_context_count": "Context Count", "msg_context_tooltip": "Number of historical chat rounds included in the prompt. Set to 0 to exclude chat history. Maximum 50 rounds. Note: More chat history means more tokens consumed.", "label_dialogue_background": "Dialogue Background", "ph_input_dialogue_background": "Describe the background of the current dialogue to help the model complete user questions, e.g.: The current dialogue is about usage questions and feature introduction of chatwiki", "msg_insert_variable": "Enter / to insert variable", "label_user_question": "User Question", "ph_select": "Please select", "label_output": "Output", "label_question_optimization_result": "Question Optimization Result"}