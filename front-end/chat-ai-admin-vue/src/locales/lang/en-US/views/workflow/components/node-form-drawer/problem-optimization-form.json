{"desc_problem_optimization": "Use LLM to optimize user questions, complete potential information gaps, and improve knowledge recall rate.", "title_llm_settings": "LLM Settings", "label_llm_model": "LLM Model", "btn_advanced_settings": "Advanced Settings", "label_prompt_role": "Prompt Role", "tip_prompt_role_desc": "When calling LLM, the prompt will be concatenated under the corresponding role.", "label_system_role_example": "System role example:", "sample_custom_prompt": "Custom prompt", "label_system_role": "System (System)", "label_user_role": "User (User)", "label_temperature": "Temperature", "tip_temperature_desc": "Lower temperature makes answers more precise. Higher temperature makes answers more diverse.", "label_max_token": "Max Tokens", "tip_max_token_desc": "Maximum tokens for question+answer. If the answer is truncated, increase this value.", "label_deep_thinking": "Deep Thinking", "tip_deep_thinking_desc": "When enabled, the LLM will use deep thinking mode.", "label_context_pair": "Context Count", "tip_context_pair_desc": "Number of historical chat rounds in the prompt. Set to 0 to exclude chat history. Maximum 50 rounds. Note: More history consumes more tokens.", "label_dialogue_background": "Dialogue Background", "msg_input_insert_variable": "Type / to insert variables", "ph_input_dialogue_background": "Describe the background of the current dialogue to help the LLM complete user questions. For example: The current dialogue is about chatwiki usage questions and feature introduction.", "label_user_question": "User Question", "label_input": "Input", "label_output": "Output", "label_question_optimization_result": "Question Optimization Result", "ph_select": "Please select"}