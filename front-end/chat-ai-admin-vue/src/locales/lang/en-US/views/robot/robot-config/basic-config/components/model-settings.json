{"title_model_settings": "Model Settings", "btn_modify": "Edit", "label_llm_model": "LLM Model", "label_temperature": "Temperature", "label_max_token": "Max Token", "label_context_pair": "Context Count", "label_show_reasoning_process": "Show Reasoning Process", "state_on": "On", "state_off": "Off", "label_prompt_role": "Prompt Role", "option_system_role": "System Role", "option_user_role": "User Role", "tooltip_temperature": "Lower temperature makes answers more precise. Higher temperature makes answers more diverse.", "tooltip_max_token": "Maximum tokens for question + answer. Increase this value if answers are truncated.", "tooltip_context_pair": "Number of historical chat rounds included in prompts. Set to 0 to exclude chat history. Maximum 50 rounds. Note: More chat history consumes more tokens.", "label_multimodal_input": "Multimodal Input", "tooltip_multimodal_input": "When enabled, the model will use deep thinking mode", "msg_model_not_support_multimodal": "The selected model does not support multimodal input", "label_deep_thinking": "Deep Thinking", "tooltip_deep_thinking": "When enabled, the model will use deep thinking mode", "tooltip_show_reasoning_process": "When enabled, API interface, chat test page, and webAPP will display or return the reasoning process"}